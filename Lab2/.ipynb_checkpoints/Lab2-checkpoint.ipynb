{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0cc6ffa-7f49-48f7-a0a9-bc7a2d96600f",
   "metadata": {},
   "source": [
    "# Lab 2 : Training an MLP (Keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2471239e-275a-4ec5-9276-9eb09b80c392",
   "metadata": {},
   "source": [
    "## 1.Load and preprocess data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99c1f714-c919-437d-8b12-c9ccea01d1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b0e66b5-45b6-467b-97a9-3b6c0d76d6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "# first we do normalization to our data to be in [0,1] and reshape it to 1D array the fed in the mlp model like this:\n",
    "#X_train = X_train.reshape(-1, 28*28) / 255.0\n",
    "#X_test = X_test.reshape(-1, 28*28) / 255.0\n",
    "# but when we will do MLP architecture with keras i will just normilize data witout reshaping to do the reshape with a new function\n",
    "y_train = to_categorical(y_train, 10) \n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72fc44d-f806-4337-a6c6-7b9c2b4f444a",
   "metadata": {},
   "source": [
    "## 2. Split the train set into Train and validations sets (20% for validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca70fa0a-e3fd-4d53-bbea-40b72e7542c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd551cd-45cf-4a10-a5c5-9b22b3af5fbf",
   "metadata": {},
   "source": [
    "# 3. Design an MLP architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f769db4d-4e5d-4286-99df-a181fef7fa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),  \n",
    "    keras.layers.Dense(128, activation='relu'),  \n",
    "    keras.layers.Dense(64, activation='relu'),   \n",
    "    keras.layers.Dense(10, activation='softmax') \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763d7b5a-5da2-4eaf-a12b-fe21ee9dff51",
   "metadata": {},
   "source": [
    "# 4. Train the designed architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7939abec-4642-4f3d-ad29-809024692d5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Argument `output` must have rank (ndim) `target.ndim - 1`. Received: target.shape=(1, 10), output.shape=(1, 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 19\u001b[0m\n\u001b[0;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mstrategy[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     15\u001b[0m              loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     16\u001b[0m              metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     18\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 19\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[0;32m     25\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m training_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m     28\u001b[0m histories\u001b[38;5;241m.\u001b[39mappend(history)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\nn.py:695\u001b[0m, in \u001b[0;36msparse_categorical_crossentropy\u001b[1;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[0;32m    689\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    690\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `output` must be at least rank 1. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    691\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    692\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    693\u001b[0m     )\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]):\n\u001b[1;32m--> 695\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    696\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `output` must have rank (ndim) `target.ndim - 1`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    697\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    698\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, output.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    699\u001b[0m     )\n\u001b[0;32m    700\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e1, e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape, output\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]):\n\u001b[0;32m    701\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e1 \u001b[38;5;241m!=\u001b[39m e2:\n",
      "\u001b[1;31mValueError\u001b[0m: Argument `output` must have rank (ndim) `target.ndim - 1`. Received: target.shape=(1, 10), output.shape=(1, 10)"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Training configurations\n",
    "strategies = [\n",
    "    {'name': 'SGD (batch=1)', 'batch_size': 1, 'optimizer': keras.optimizers.SGD(learning_rate=0.01)},\n",
    "    {'name': 'Mini-batch SGD (64)', 'batch_size': 64, 'optimizer': keras.optimizers.SGD(learning_rate=0.01)},\n",
    "    {'name': 'Batch SGD', 'batch_size': len(X_train), 'optimizer': keras.optimizers.SGD(learning_rate=0.01)}\n",
    "]\n",
    "\n",
    "# Train and plot learning curves\n",
    "histories = []\n",
    "training_times = []\n",
    "\n",
    "for strategy in strategies:\n",
    "    model.compile(optimizer=strategy['optimizer'],\n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=strategy['batch_size'],\n",
    "        epochs=50,\n",
    "        validation_data=(X_val, y_val),\n",
    "        verbose=0\n",
    "    )\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    histories.append(history)\n",
    "    training_times.append(training_time)\n",
    "    print(f\"Completed {strategy['name']} in {training_time:.2f}s\")\n",
    "\n",
    "# Plot learning curves\n",
    "def plot_curves(history, title):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "    plt.title(title + ' Loss')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "    plt.title(title + ' Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "for i, history in enumerate(histories):\n",
    "    plot_curves(history, strategies[i]['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5a0cc0-e3c0-4109-9bf2-9f2628f34381",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hafsa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# 1. Load and preprocess MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize and reshape\n",
    "X_train = X_train.reshape(-1, 28*28).astype('float32') / 255.0\n",
    "X_test = X_test.reshape(-1, 28*28).astype('float32') / 255.0\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# 2. Split train set into train and validation (80-20)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to create the MLP model\n",
    "def create_model(optimizer):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(784,)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# 4. Train with different batch strategies\n",
    "def train_and_plot(optimizer, batch_size, epochs=50, label=''):\n",
    "    model = create_model(optimizer)\n",
    "    start = time.time()\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                        epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    duration = time.time() - start\n",
    "    \n",
    "    # Plot learning curves\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "    plt.title(f'Loss Curves - {label}')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "    plt.title(f'Accuracy Curves - {label}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return history, duration\n",
    "\n",
    "# Point 4: SGD, Mini-batch (64), Batch SGD\n",
    "sgd_optimizer = SGD(learning_rate=0.01)\n",
    "hist_sgd, time_sgd = train_and_plot(sgd_optimizer, batch_size=1, label='SGD (batch=1)')\n",
    "hist_minibatch, time_minibatch = train_and_plot(sgd_optimizer, batch_size=64, label='Mini-batch SGD (64)')\n",
    "hist_batch, time_batch = train_and_plot(sgd_optimizer, batch_size=len(X_train), label='Batch SGD')\n",
    "\n",
    "# 5. Compare Mini-batch SGD variants\n",
    "optimizers = {\n",
    "    'Mini-batch SGD': SGD(learning_rate=0.01),\n",
    "    'Mini-batch SGD + Decay': SGD(learning_rate=0.01, decay=1e-6),\n",
    "    'Mini-batch SGD + Decay + Momentum': SGD(learning_rate=0.01, decay=1e-6, momentum=0.9)\n",
    "}\n",
    "\n",
    "histories_decay = {}\n",
    "times_decay = {}\n",
    "for name, opt in optimizers.items():\n",
    "    hist, t = train_and_plot(opt, batch_size=64, label=name)\n",
    "    histories_decay[name] = hist\n",
    "    times_decay[name] = t\n",
    "\n",
    "# 6. Compare different optimizers\n",
    "optimizers_compare = {\n",
    "    'SGD (lr=0.01)': SGD(learning_rate=0.01),\n",
    "    'Adam (lr=0.001)': Adam(learning_rate=0.001),\n",
    "    'RMSprop (lr=0.001)': RMSprop(learning_rate=0.001)\n",
    "}\n",
    "\n",
    "histories_opt = {}\n",
    "times_opt = {}\n",
    "for name, opt in optimizers_compare.items():\n",
    "    hist, t = train_and_plot(opt, batch_size=64, label=name)\n",
    "    histories_opt[name] = hist\n",
    "    times_opt[name] = t\n",
    "\n",
    "# 7. Save the best model based on validation accuracy\n",
    "best_val_accuracy = max([max(hist.history['val_accuracy']) for hist in histories_opt.values()])\n",
    "for name, hist in histories_opt.items():\n",
    "    if max(hist.history['val_accuracy']) == best_val_accuracy:\n",
    "        model = create_model(optimizers_compare[name])\n",
    "        model.fit(X_train, y_train, epochs=50, batch_size=64, verbose=0)  # Retrain to get the model\n",
    "        model.save('best_model.h5')\n",
    "        break\n",
    "\n",
    "# Load the best model and evaluate\n",
    "best_model = load_model('best_model.h5')\n",
    "test_loss, test_acc = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Predict on some test samples\n",
    "sample_images = X_test[:5]\n",
    "predictions = best_model.predict(sample_images)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "print(\"Predicted Classes:\", predicted_classes)\n",
    "print(\"True Classes:\", np.argmax(y_test[:5], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2778fba3-5573-4a2e-9fa5-f096f521efd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
